{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always run the following 3 cells first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.sequence_generator import generate_examples\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import harvard_transformer as tr\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PARAMETERS #####\n",
    "dataset_file = \"ten_tokens_explicit_singular_data.txt\"\n",
    "vocabulary_file = 'datasets/ten_tokens_explicit.txt'\n",
    "operation_type = 'singular'\n",
    "transition_type = 'explicit'\n",
    "\n",
    "##### MODEL PARAMETERS #####\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "hidden_size = 16 # The hidden size MUST be divisible by the number of heads and even\n",
    "model_save = 'transformer_hid_'+str(hidden_size)+'_heads_'+str(num_heads)+'_lyrs_'+str(num_layers)+'.mdl'\n",
    "\n",
    "##### TRAINING PARAMETERS #####\n",
    "batch_size = 24\n",
    "num_batches = 100\n",
    "num_epochs = 100\n",
    "print_frequency = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(vocabulary, batch_size, num_batches, Ks=[2,4,5,7]):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for btch in range(num_batches):\n",
    "        selected_ks = np.random.choice(Ks, batch_size)\n",
    "\n",
    "        pad = len(vocabulary)\n",
    "        srcs = []\n",
    "        tgts = []\n",
    "        for lk in selected_ks:\n",
    "            output = generate_examples(transition_type = transition_type, \\\n",
    "                                        operation_type = operation_type, \\\n",
    "                                        vocabulary = vocabulary, \\\n",
    "                                        k = lk, \\\n",
    "                                        num_examples=1).strip()\n",
    "\n",
    "            training_line, target = output.split(';')\n",
    "            training_sequence = training_line.split(' ')\n",
    "\n",
    "            src = []\n",
    "            for char in training_sequence:\n",
    "                src.append(vocabulary.index(char))\n",
    "\n",
    "            for padding_char in range(20 - len(training_sequence)):\n",
    "                src.append(pad)\n",
    "\n",
    "            tgt = [pad, vocabulary.index(target)]\n",
    "            # for padding_char in range(20 - len(tgt)):\n",
    "            #     tgt.append(pad)\n",
    "\n",
    "            srcs.append(src)\n",
    "            tgts.append(tgt)\n",
    "\n",
    "        srcs = torch.from_numpy(np.array(srcs))\n",
    "        tgts = torch.from_numpy(np.array(tgts))\n",
    "\n",
    "        # data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        # data[:, 0] = 1\n",
    "        srcs = Variable(srcs, requires_grad=False)\n",
    "        tgts = Variable(tgts, requires_grad=False)\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        yield tr.Batch(srcs, tgts, pad)\n",
    "\n",
    "def data_sample(vocabulary, Ks=[2,4,5,7]):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    pad = len(vocabulary)\n",
    "    output = generate_examples(transition_type = transition_type, \\\n",
    "                               operation_type = operation_type, \\\n",
    "                               vocabulary = vocabulary, \\\n",
    "                               k = np.random.choice(Ks), \\\n",
    "                               num_examples=1).strip()\n",
    "\n",
    "    training_line, target = output.split(';')\n",
    "    training_sequence = training_line.split(' ')\n",
    "\n",
    "    src = []\n",
    "    for char in training_sequence:\n",
    "        src.append(vocabulary.index(char))\n",
    "\n",
    "    for padding_char in range(20 - len(training_sequence)):\n",
    "        src.append(pad)\n",
    "\n",
    "    tgt = [pad, vocabulary.index(target)]\n",
    "            # for padding_char in range(20 - len(tgt)):\n",
    "            #     tgt.append(pad)\n",
    "\n",
    "    src = Variable(torch.from_numpy(np.array(src)), requires_grad=False)\n",
    "    tgt = Variable(torch.from_numpy(np.array(tgt)), requires_grad=False)\n",
    "        # import pdb; pdb.set_trace()\n",
    "    return src, tgt\n",
    "\n",
    "vocabulary = []\n",
    "with open(vocabulary_file) as file:\n",
    "    for line in file:\n",
    "        vocabulary.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell only loads the model. If the parameters specified prebiously describe the model you wish to load then run this following cell to load the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD MODEL #####\n",
    "model = tr.make_transformer(src_vocab=len(vocabulary)+1, \\\n",
    "                        tgt_vocab=len(vocabulary)+1, \\\n",
    "                        N=num_layers, \\\n",
    "                        d_model=hidden_size, \\\n",
    "                        d_ff=4*hidden_size, \\\n",
    "                        h=num_heads, \\\n",
    "                        dropout=0.0)\n",
    "model.load_state_dict(torch.load(model_save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains a new model using the parameters set above. The model will be save ever `print_frequency` epochs under the name set in `model_save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAIN MODEL #####\n",
    "model = tr.make_transformer(src_vocab=len(vocabulary)+1, \\\n",
    "                        tgt_vocab=len(vocabulary)+1, \\\n",
    "                        N=num_layers, \\\n",
    "                        d_model=hidden_size, \\\n",
    "                        d_ff=4*hidden_size, \\\n",
    "                        h=num_heads, \\\n",
    "                        dropout=0.0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_opt = tr.NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "print('Training Transformer')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    tr.run_epoch(data_generator(vocabulary, batch_size, num_batches), \\\n",
    "                model, \\\n",
    "              tr.SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "    \n",
    "    if (epoch + 1) % print_frequency == 0:\n",
    "        print('Epoch::'+str(epoch+1))\n",
    "        model.eval()\n",
    "        print(tr.run_epoch(data_generator(vocabulary, batch_size, 1), \\\n",
    "                    model, \\\n",
    "                  tr.SimpleLossCompute(model.generator, criterion, None)))\n",
    "        torch.save(model.state_dict(), model_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell tests the model. Be sure to have either a model trained or loaded using one of the previous cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TEST MODEL #####\n",
    "model.eval()\n",
    "for k in range(2,11):\n",
    "    correct = 0\n",
    "    for ex in range(2000):\n",
    "        src, tgt = data_sample(vocabulary, Ks=[k])\n",
    "        src = src.reshape(1,-1)\n",
    "        true = tgt[1].item()\n",
    "        \n",
    "        src_mask = (src != len(vocabulary)).unsqueeze(-2)\n",
    "        out = tr.greedy_decode(model, src, src_mask, max_len=2, start_symbol=len(vocabulary))\n",
    "        \n",
    "        pred = out[0][1].item()\n",
    "\n",
    "        # print(pred, true)\n",
    "        if pred == true:\n",
    "            # print('CORR')\n",
    "            correct += 1\n",
    "    # import pdb; pdb.set_trace()\n",
    "    print('Dataset L'+str(k)+' Accuracy: '+str(round(correct/2000*100, 2)) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
